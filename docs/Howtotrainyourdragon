# ğŸ“ Training Guide

Complete guide for data management and model training with BugPredict AI.

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Training Data Sources](#training-data-sources)
3. [Data Requirements](#data-requirements)
4. [Training from CSV](#training-from-csv)
5. [Training from JSON](#training-from-json)
6. [Training from Database](#training-from-database)
7. [Training with Mock Data](#training-with-mock-data)
8. [Understanding Model Output](#understanding-model-output)
9. [Model Performance](#model-performance)
10. [Best Practices](#best-practices)
11. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

BugPredict AI trains machine learning models on vulnerability data to predict likely vulnerabilities on target domains. The system supports multiple data sources and provides flexible training options.

**What Gets Trained:**
- **Vulnerability Classifier** - Predicts which of 10 vulnerability types are likely present
- **Severity Predictor** - Predicts severity level (critical/high/medium/low)
- **Chain Detector** - Identifies potential vulnerability chains (pattern-based)

**Training Workflow:**
1. Choose data source (CSV, JSON, Database, or Mock)
2. Run training script
3. Models are saved to `data/models/`
4. Use models to generate Nuclei templates

---

## ğŸ“Š Training Data Sources

BugPredict AI supports 4 data sources:

| Source | Best For | Difficulty | Setup Time |
|--------|----------|-----------|------------|
| **Mock Data** | Testing, learning, demos | â­ Easy | 1 minute |
| **CSV Files** | Small datasets, spreadsheets | â­ Easy | 5 minutes |
| **JSON Files** | Structured data, APIs | â­â­ Medium | 5 minutes |
| **SQL Databases** | Large datasets, production | â­â­â­ Advanced | 15-30 minutes |

**Recommendation:** Start with **Mock Data** to learn the system, then move to real data.

---

## ğŸ“‹ Data Requirements

### Minimum Data Quality

| Records | Accuracy | Use Case |
|---------|----------|----------|
| 10-50 | ~25-50% | Testing only |
| 100-500 | ~70-80% | Development |
| 500-1000 | ~85-90% | Production |
| 1000+ | ~90-95% | Professional use |

### Required Fields

Every vulnerability report must include:

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `report_id` | String | Unique identifier | "VULN-001" |
| `target_domain` | String | Target domain | "example.com" |
| `target_company` | String | Company name | "Example Corp" |
| `vulnerability_type` | String | Vulnerability type | "SQL Injection" |
| `severity` | String | Severity level | "high" |
| `cvss_score` | Float | CVSS score (0-10) | 7.5 |

### Optional Fields (Improve Accuracy)

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `tech_stack` | String/Array | Technologies used | "React,Node.js,MySQL" |
| `description` | String | Vulnerability details | "SQL injection in login" |
| `endpoint` | String | Affected endpoint | "/api/login" |
| `http_method` | String | HTTP method | "POST" |
| `bounty_amount` | Float | Bounty paid | 500.0 |

### Supported Vulnerability Types

Train your models on these 10 vulnerability types:

1. SQL Injection
2. XSS (Cross-Site Scripting)
3. SSRF (Server-Side Request Forgery)
4. IDOR (Insecure Direct Object Reference)
5. CSRF (Cross-Site Request Forgery)
6. Authentication Bypass
7. RCE (Remote Code Execution)
8. XXE (XML External Entity)
9. Path Traversal
10. Information Disclosure

### Supported Severity Levels

- `critical`
- `high`
- `medium`
- `low`

---

## ğŸ“„ Training from CSV

### CSV Format

Create a CSV file with vulnerability data:
```csv
report_id,target_domain,target_company,vulnerability_type,severity,cvss_score,tech_stack,description,endpoint,http_method,bounty_amount
001,example.com,Example Corp,SQL Injection,high,7.5,"React,Node.js,MySQL",SQL injection in login form,/api/login,POST,500
002,test.io,Test Inc,XSS,medium,5.0,"Angular,Java",Stored XSS in comments,/api/comments,POST,250
003,demo.app,Demo LLC,IDOR,high,8.2,"Vue.js,PostgreSQL",IDOR in user API,/api/users/{id},GET,750
```

### Validate CSV

Check if your CSV is valid before training:
```bash
python scripts/train_from_csv.py --input your_data.csv --validate-only
```

**Expected output:**
```
======================================================================
BugPredict AI - CSV Training
======================================================================

Validating CSV file: your_data.csv
âœ“ CSV is valid
  Rows: 100
  Columns: ['report_id', 'target_domain', 'target_company', ...]

âœ“ Validation complete (--validate-only mode)
```

### Train from CSV
```bash
python scripts/train_from_csv.py --input your_data.csv
```

**Expected output:**
```
======================================================================
BugPredict AI - CSV Training
======================================================================

Validating CSV file: your_data.csv
âœ“ CSV is valid
  Rows: 100

Importing vulnerability reports...
âœ“ Imported 100 reports

Preprocessing reports...
âœ“ Preprocessed 100 reports

Engineering features...
âœ“ Features engineered (28 features)

Training models...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training vulnerability classifier...
âœ“ Vulnerability classifier trained (Accuracy: 92.5%)

Training severity predictor...
âœ“ Severity predictor trained (Accuracy: 98.0%)

Training chain detector...
âœ“ Chain detector trained

Saving models to: data/models
âœ“ Models saved

======================================================================
TRAINING COMPLETE
======================================================================

Models saved to: C:\...\BugPredict-AI\data\models
```

### Custom Output Directory
```bash
python scripts/train_from_csv.py --input data.csv --output-dir custom_models/
```

---

## ğŸ“‹ Training from JSON

### JSON Format

Create a JSON file with vulnerability data:
```json
[
  {
    "report_id": "001",
    "target_domain": "example.com",
    "target_company": "Example Corp",
    "vulnerability_type": "SQL Injection",
    "severity": "high",
    "cvss_score": 7.5,
    "tech_stack": ["React", "Node.js", "MySQL"],
    "description": "SQL injection in login form",
    "endpoint": "/api/login",
    "http_method": "POST",
    "bounty_amount": 500
  },
  {
    "report_id": "002",
    "target_domain": "test.io",
    "target_company": "Test Inc",
    "vulnerability_type": "XSS",
    "severity": "medium",
    "cvss_score": 5.0,
    "tech_stack": ["Angular", "Java"],
    "description": "Stored XSS in comments",
    "endpoint": "/api/comments",
    "http_method": "POST",
    "bounty_amount": 250
  }
]
```

### Validate JSON
```bash
python scripts/train_from_json.py --input your_data.json --validate-only
```

**Expected output:**
```
======================================================================
BugPredict AI - JSON Training
======================================================================

Validating JSON file: your_data.json
âœ“ JSON is valid
  Reports: 50

âœ“ Validation complete (--validate-only mode)
```

### Train from JSON
```bash
python scripts/train_from_json.py --input your_data.json
```

### Custom Output Directory
```bash
python scripts/train_from_json.py --input data.json --output-dir custom_models/
```

---

## ğŸ—„ï¸ Training from Database

### Prerequisites

- Database must be set up (see [DATABASE.md](DATABASE.md))
- Required Python package installed for your database
- Table with vulnerability data exists

### Validate Database Connection
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --validate-only
```

### List Available Tables
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --list-tables
```

### View Table Schema
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --schema vulnerability_reports
```

### Train from Database
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --table vulnerability_reports
```

### SQLite Example
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --table vulnerability_reports
```

### PostgreSQL Example
```bash
python scripts/train_from_database.py \
  --db "postgresql://user:pass@localhost:5432/vulndb" \
  --table vulnerability_reports
```

### MySQL Example
```bash
python scripts/train_from_database.py \
  --db "mysql://user:pass@localhost:3306/vulndb" \
  --table vulnerability_reports
```

### SQL Server Example
```bash
python scripts/train_from_database.py \
  --db "mssql+pyodbc://user:pass@localhost/vulndb?driver=ODBC+Driver+17+for+SQL+Server" \
  --table vulnerability_reports
```

### Advanced: Filter Data

Train only on high-severity vulnerabilities:
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --table vulnerability_reports \
  --where "severity='high' OR severity='critical'"
```

Train only on recent data:
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --table vulnerability_reports \
  --where "reported_date >= '2025-01-01'"
```

### Advanced: Limit Records

Train on first 500 records (faster iteration):
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --table vulnerability_reports \
  --limit 500
```

Combine filter and limit:
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/vulns.db" \
  --table vulnerability_reports \
  --where "severity='critical'" \
  --limit 100
```

---

## ğŸ² Training with Mock Data

### Why Use Mock Data?

- âœ… **No real vulnerability data needed**
- âœ… **Perfect for testing and learning**
- âœ… **Generate any amount of data instantly**
- âœ… **Realistic data distribution**

### Generate Mock Database
```bash
# Generate 100 mock vulnerability reports
python scripts/create_mock_database.py --reports 100

# Generate 1000 reports
python scripts/create_mock_database.py --reports 1000

# Custom output location
python scripts/create_mock_database.py --reports 500 --output data/test_vulns.db
```

**Expected output:**
```
======================================================================
Mock Vulnerability Database Generator
======================================================================

Creating 100 mock vulnerability reports...

âœ“ Created database: data/mock_vulns.db
Total reports: 100

Vulnerability breakdown:
  SQL Injection                  12
  XSS                            11
  SSRF                           10
  IDOR                            9
  CSRF                           10
  Authentication Bypass           8
  RCE                             9
  XXE                            11
  Path Traversal                 10
  Information Disclosure         10

Severity breakdown:
  high       28
  medium     26
  critical   24
  low        22
```

### Train on Mock Data
```bash
python scripts/train_from_database.py \
  --db "sqlite:///data/mock_vulns.db" \
  --table vulnerability_reports
```

### Quick Test Workflow
```bash
# 1. Generate mock data
python scripts/create_mock_database.py --reports 100

# 2. Train models
python scripts/train_from_database.py --db "sqlite:///data/mock_vulns.db" --table vulnerability_reports

# 3. Generate templates
python scripts/generate_nuclei_templates.py --target example.com

# Total time: ~30 seconds
```

---

## ğŸ“Š Understanding Model Output

### Training Output Explained
```
Training vulnerability classifier...
âœ“ Vulnerability classifier trained (Accuracy: 92.5%)
```

**What this means:**
- The model correctly predicted vulnerability types 92.5% of the time on test data
- Higher is better (70%+ is good, 90%+ is excellent)
```
Training severity predictor...
âœ“ Severity predictor trained (Accuracy: 98.0%)
```

**What this means:**
- The model correctly predicted severity levels 98% of the time
- Severity prediction is usually more accurate than vulnerability classification
```
Training chain detector...
âœ“ Chain detector trained
```

**What this means:**
- Pattern-based detector is ready to identify vulnerability chains
- No accuracy score (rule-based, not ML)

### Saved Models

After training, 4 files are saved to `data/models/`:

| File | Purpose | Size |
|------|---------|------|
| `vulnerability_classifier.pkl` | Predicts vulnerability types | ~1-5 MB |
| `severity_predictor.pkl` | Predicts severity levels | ~1-5 MB |
| `chain_detector.pkl` | Detects vulnerability chains | ~100 KB |
| `feature_engineer.pkl` | Transforms input data to features | ~500 KB |

### Model Files Explained
```
data/models/
â”œâ”€â”€ vulnerability_classifier.pkl    # Random Forest model + label encoder
â”œâ”€â”€ severity_predictor.pkl          # Random Forest model + label encoder
â”œâ”€â”€ chain_detector.pkl              # Pattern-based detector
â””â”€â”€ feature_engineer.pkl            # Feature transformation pipeline
```

**These files are required** for generating Nuclei templates.

---

## ğŸ“ˆ Model Performance

### Accuracy by Dataset Size

| Records | Expected Accuracy | Training Time |
|---------|------------------|---------------|
| 10-50 | 25-50% | 5 seconds |
| 100 | 70-80% | 10 seconds |
| 500 | 85-90% | 30 seconds |
| 1000 | 90-95% | 60 seconds |
| 5000+ | 95-98% | 5-10 minutes |

### Factors Affecting Accuracy

**Good for accuracy:**
- âœ… More training data (1000+ records)
- âœ… Diverse vulnerability types
- âœ… High-quality, detailed descriptions
- âœ… Complete tech stack information
- âœ… Accurate CVSS scores
- âœ… Balanced distribution across vulnerability types

**Bad for accuracy:**
- âŒ Too little data (<100 records)
- âŒ Only one or two vulnerability types
- âŒ Missing or incomplete data
- âŒ Inconsistent severity labeling
- âŒ Duplicate reports
- âŒ Poor quality descriptions

### Training Time Benchmarks

**Test environment:** Intel i7-9700K, 16GB RAM, Windows 11

| Dataset Size | CSV Import | Training | Total Time |
|-------------|-----------|----------|------------|
| 100 reports | 1 sec | 5 sec | ~6 seconds |
| 500 reports | 3 sec | 20 sec | ~23 seconds |
| 1000 reports | 5 sec | 45 sec | ~50 seconds |
| 5000 reports | 15 sec | 4 min | ~4.5 minutes |
| 10000 reports | 30 sec | 8 min | ~9 minutes |

---

## âœ… Best Practices

### Data Collection

1. **Collect diverse data**
   - Include all 10 vulnerability types
   - Mix of severity levels
   - Different tech stacks

2. **Ensure data quality**
   - Remove duplicates
   - Verify severity labels match CVSS scores
   - Complete required fields

3. **Use realistic data**
   - Real vulnerability reports are best
   - Mock data is fine for testing only

### Training Strategy

1. **Start small, scale up**
```bash
   # Start with 100 reports
   python scripts/train_from_database.py --db "..." --table reports --limit 100
   
   # Then try 500
   python scripts/train_from_database.py --db "..." --table reports --limit 500
   
   # Finally train on all data
   python scripts/train_from_database.py --db "..." --table reports
```

2. **Iterate and improve**
   - Train â†’ Test â†’ Add more data â†’ Retrain
   - Focus on vulnerability types with low accuracy

3. **Keep models updated**
   - Retrain monthly with new data
   - Archive old models before retraining

### Data Organization

**Recommended directory structure:**
```
data/
â”œâ”€â”€ raw/                    # Original CSV/JSON files
â”‚   â”œâ”€â”€ 2025-01.csv
â”‚   â”œâ”€â”€ 2025-02.csv
â”‚   â””â”€â”€ combined.csv
â”œâ”€â”€ databases/              # SQLite databases
â”‚   â”œâ”€â”€ production.db
â”‚   â””â”€â”€ test.db
â””â”€â”€ models/                 # Trained models
    â”œâ”€â”€ vulnerability_classifier.pkl
    â”œâ”€â”€ severity_predictor.pkl
    â”œâ”€â”€ chain_detector.pkl
    â””â”€â”€ feature_engineer.pkl
```

### Version Control for Models
```bash
# Backup models before retraining
cp -r data/models data/models_backup_2025-02-05

# Or use dated folders
mkdir data/models_2025-02-05
cp data/models/*.pkl data/models_2025-02-05/
```

---

## ğŸ› Troubleshooting

### Issue: Low Accuracy (<70%)

**Possible causes:**
- Not enough training data
- Imbalanced dataset (too much of one vulnerability type)
- Poor data quality

**Solutions:**
```bash
# Check data distribution
python scripts/train_from_database.py --db "..." --list-tables
python scripts/train_from_database.py --db "..." --schema vulnerability_reports

# Add more diverse data
# Remove duplicates
# Verify severity labels are correct
```

---

### Issue: Training Takes Too Long

**Solutions:**
```bash
# Use smaller dataset during development
python scripts/train_from_database.py --db "..." --table reports --limit 500

# Use mock data for quick testing
python scripts/create_mock_database.py --reports 100
```

---

### Issue: Out of Memory

**Solutions:**
```bash
# Reduce dataset size
python scripts/train_from_database.py --db "..." --table reports --limit 1000

# Close other applications
# Use system with more RAM
```

---

### Issue: Models Not Saving

**Check:**
```bash
# Does output directory exist?
ls -la data/models/  # Linux/Mac
dir data\models\     # Windows

# Create if missing
mkdir -p data/models  # Linux/Mac
New-Item -ItemType Directory -Path data\models -Force  # Windows
```

---

### Issue: Import Errors

**For CSV:**
```bash
# Validate first
python scripts/train_from_csv.py --input data.csv --validate-only

# Check file encoding (should be UTF-8)
# Check for missing commas
# Verify column names match exactly
```

**For JSON:**
```bash
# Validate first
python scripts/train_from_json.py --input data.json --validate-only

# Use a JSON validator: https://jsonlint.com
# Check for trailing commas
# Verify proper JSON array format
```

**For Database:**
```bash
# Validate connection
python scripts/train_from_database.py --db "..." --validate-only

# List tables
python scripts/train_from_database.py --db "..." --list-tables

# Check schema
python scripts/train_from_database.py --db "..." --schema TABLE_NAME
```

---

## ğŸ“š Quick Reference

### Training Commands
```bash
# CSV
python scripts/train_from_csv.py --input data.csv

# JSON
python scripts/train_from_json.py --input data.json

# Database
python scripts/train_from_database.py --db "CONNECTION_STRING" --table TABLE_NAME

# Mock Data
python scripts/create_mock_database.py --reports 1000
python scripts/train_from_database.py --db "sqlite:///data/mock_vulns.db" --table vulnerability_reports
```

### Validation Commands
```bash
# CSV
python scripts/train_from_csv.py --input data.csv --validate-only

# JSON
python scripts/train_from_json.py --input data.json --validate-only

# Database
python scripts/train_from_database.py --db "..." --validate-only
```

### Common Flags
```bash
--input FILE           # Input CSV/JSON file
--db CONNECTION        # Database connection string
--table TABLE          # Database table name
--output-dir DIR       # Output directory for models (default: data/models)
--validate-only        # Only validate, don't train
--limit N              # Limit to N records
--where "SQL"          # SQL WHERE clause filter
```

---

## ğŸ¯ Next Steps

After training your models:

1. **[Generate Nuclei Templates](SCRIPTS.md#generate-nuclei-templates)** - Create templates from trained models
2. **Test Templates** - Run Nuclei scan on test targets
3. **Evaluate Results** - Assess accuracy and adjust training data
4. **Retrain** - Add more data and retrain for better accuracy

---

**Training complete! Ready to generate Nuclei templates.** ğŸ¯
